{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f99bd171",
      "metadata": {
        "id": "f99bd171"
      },
      "source": [
        "# 03 — Gradio QA Demo (Retrieval + BioGPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a7092323",
      "metadata": {
        "id": "a7092323"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers datasets gradio scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone once\n",
        "!rm -rf pubmedqa-llm-bot\n",
        "!git clone https://github.com/AnnaJazayeri/pubmedqa-llm-bot.git\n",
        "%cd /content/pubmedqa-llm-bot\n",
        "\n",
        "# install dependencies\n",
        "!pip -q install -r requirements.txt\n",
        "\n",
        "# make sure Python can see the project root so `src` imports work\n",
        "import sys\n",
        "if '/content/pubmedqa-llm-bot' not in sys.path:\n",
        "    sys.path.append('/content/pubmedqa-llm-bot')\n",
        "\n",
        "# then jump into notebooks folder if you want to open/run them there\n",
        "%cd notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epeJn-xQsRoy",
        "outputId": "7d07c761-1657-4a6a-bbab-a2b26be69998"
      },
      "id": "epeJn-xQsRoy",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pubmedqa-llm-bot\n",
            "/content/pubmedqa-llm-bot/notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "24808001",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "24808001",
        "outputId": "c2060eaf-274f-44b2-f99b-2699f68c8b7d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "string indices must be integers, not 'str'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-950229635.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load a small corpus (contexts) to retrieve from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pubmed_qa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pqa_labeled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_ctxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# cap for speed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleTfidfRetriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ctxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import gradio as gr\n",
        "from src.retrieval import SimpleTfidfRetriever\n",
        "from src.utils import PROMPT_TEMPLATE, normalize_label\n",
        "\n",
        "# Load a small corpus (contexts) to retrieve from\n",
        "ds = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
        "train_ctxs = [ex[\"context\"] for ex in ds[\"train\"][:2000]]  # cap for speed\n",
        "retriever = SimpleTfidfRetriever(train_ctxs)\n",
        "\n",
        "# Load BioGPT (base)\n",
        "model_name = \"microsoft/biogpt\"\n",
        "tok = AutoTokenizer.from_pretrained(model_name)\n",
        "gen = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "pipe = pipeline(\"text-generation\", model=gen, tokenizer=tok, device_map=\"auto\", max_new_tokens=64)\n",
        "\n",
        "def answer_question(question):\n",
        "    top = retriever.topk(question, k=3)\n",
        "    idxs = [i for i,_ in top]\n",
        "    contexts = retriever.fetch(idxs)\n",
        "    merged = \"\\n\\n\".join(contexts)\n",
        "    prompt = PROMPT_TEMPLATE.format(question=question, context=merged[:2000])\n",
        "    out = pipe(prompt, do_sample=False)[0][\"generated_text\"]\n",
        "    ans = out.split(\"Answer:\")[-1].strip()\n",
        "    return ans\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=gr.Textbox(lines=3, placeholder=\"Ask a biomedical question...\"),\n",
        "    outputs=gr.Textbox(label=\"LLM Answer (Yes/No/Maybe)\"),\n",
        "    title=\"PubMedQA — BioGPT (Demo)\",\n",
        "    description=\"Retrieval-augmented QA for Yes/No/Maybe answers. Uses a small TF–IDF retriever and BioGPT generation.\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)  # Colab will print a public link"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}